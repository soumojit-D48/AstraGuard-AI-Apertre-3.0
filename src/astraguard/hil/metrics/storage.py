"""Persistent metrics storage for AstraGuard HIL (Hardware-in-the-Loop) system.

This module provides functionality to store, retrieve, and compare latency metrics
collected during HIL testing runs. It handles both aggregated statistics and raw
measurement data, enabling performance analysis and regression detection.
"""

import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any
from datetime import datetime

from astraguard.hil.metrics.latency import LatencyCollector


class MetricsStorage:
    """Manages persistent storage of latency metrics for HIL testing runs.

    This class provides methods to save latency measurements to disk, load previously
    saved metrics, compare performance between different runs, and retrieve recent
    test runs.

    Attributes:
        run_id (str): Unique identifier for the current testing run.
        metrics_dir (Path): Directory path where metrics for this run are stored.
    """

    def __init__(self, run_id: str, results_dir: str = "astraguard/hil/results"):
        """
        Initialize metrics storage.

        Args:
            run_id (str): Unique identifier for this run.
            results_dir (str, optional): Base directory for results. Defaults to "astraguard/hil/results".
        """
        try:
            self.run_id = run_id
            self.metrics_dir = Path(results_dir) / run_id
            self.metrics_dir.mkdir(parents=True, exist_ok=True)
        except (OSError, PermissionError) as e:
            logging.error(f"Failed to initialize MetricsStorage for run {run_id}: {e}")
            raise
        except Exception as e:
            logging.error(f"Unexpected error initializing MetricsStorage for run {run_id}: {e}")
            raise

    async def save_latency_stats(self, collector: LatencyCollector) -> Dict[str, str]:
        """
        Save aggregated and raw latency metrics to disk.

        Args:
            collector (LatencyCollector): LatencyCollector instance containing measurements to save.

        Returns:
            Dict[str, str]: Dictionary with paths to saved files, containing 'summary' and 'raw' keys
                pointing to the JSON summary and CSV raw data files respectively.
        """
        try:
            stats = collector.get_stats()
            summary = collector.get_summary()

            # Summary JSON with all statistics
            summary_dict = {
                "run_id": self.run_id,
                "timestamp": datetime.now().isoformat(),
                "total_measurements": len(collector.measurements),
                "measurement_types": summary.get("measurement_types", {}),
                "stats": stats,
                "stats_by_satellite": summary.get("stats_by_satellite", {}),
            }

            summary_path = self.metrics_dir / "latency_summary.json"
            await asyncio.to_thread(summary_path.write_text, json.dumps(summary_dict, indent=2, default=str))

            # Raw CSV for external analysis
            csv_path = self.metrics_dir / "latency_raw.csv"
            await asyncio.to_thread(collector.export_csv, str(csv_path))

            return {"summary": str(summary_path), "raw": str(csv_path)}
        except (OSError, PermissionError) as e:
            logging.error(f"Failed to save latency stats for run {self.run_id}: {e}")
            raise
        except Exception as e:
            logging.error(f"Unexpected error saving latency stats for run {self.run_id}: {e}")
            raise

    async def get_run_metrics(self) -> Dict[str, Any]:
        """
        Load metrics from this run asynchronously.

        Returns:
            Dict[str, Any] or None: Parsed metrics dictionary containing run statistics,
                or None if the metrics file is not found or cannot be loaded.
        """
        summary_path = self.metrics_dir / "latency_summary.json"
        if not await asyncio.to_thread(summary_path.exists):
            return None

        try:
            content = await asyncio.to_thread(summary_path.read_text)
            return json.loads(content)
        except (OSError, PermissionError, IsADirectoryError) as e:
            logging.error(f"Failed to read metrics file {summary_path}: {e}")
            return None
        except json.JSONDecodeError as e:
            logging.error(f"Failed to parse JSON from {summary_path}: {e}")
            return None
        except Exception as e:
            logging.error(f"Unexpected error loading metrics from {summary_path}: {e}")
            return None

    def compare_runs(self, other_run_id: str) -> Dict[str, Any]:
        """
        Compare metrics between two runs.

        Args:
            other_run_id (str): Other run ID to compare against.

        Returns:
            Dict[str, Any]: Comparison results containing metrics differences between the two runs.
                Includes error information if metrics cannot be loaded.
        """
        other_storage = MetricsStorage(other_run_id)
        other_metrics = other_storage.get_run_metrics()

        if other_metrics is None:
            return {"error": f"Could not load metrics for run {other_run_id}", "metrics": {}}

        this_metrics = self.get_run_metrics()
        if this_metrics is None:
            return {"error": f"Could not load metrics for run {self.run_id}", "metrics": {}}

        comparison = {
            "run1": self.run_id,
            "run2": other_run_id,
            "timestamp": datetime.now().isoformat(),
            "metrics": {},
        }

        # Compare each metric type
        this_stats = this_metrics.get("stats", {})
        other_stats = other_metrics.get("stats", {})

        for metric_type in set(list(this_stats.keys()) + list(other_stats.keys())):
            this_data = this_stats.get(metric_type, {})
            other_data = other_stats.get(metric_type, {})

            if not this_data or not other_data:
                continue

            comparison["metrics"][metric_type] = {
                "this_mean_ms": this_data.get("mean_ms", 0),
                "other_mean_ms": other_data.get("mean_ms", 0),
                "diff_ms": this_data.get("mean_ms", 0) - other_data.get("mean_ms", 0),
                "this_p95_ms": this_data.get("p95_ms", 0),
                "other_p95_ms": other_data.get("p95_ms", 0),
            }

        return comparison

    @staticmethod
    async def get_recent_runs(
        results_dir: str = "astraguard/hil/results", limit: int = 10
    ) -> list:
        """
        Get recent metric runs asynchronously.

        Args:
            results_dir (str, optional): Base results directory. Defaults to "astraguard/hil/results".
            limit (int, optional): Maximum number of runs to return. Defaults to 10.

        Returns:
            list: List of recent run IDs, sorted by most recent first.
        """
        try:
            results_path = Path(results_dir)
            if not await asyncio.to_thread(results_path.exists):
                return []

            # Find directories with latency metrics
            runs = []
            for run_dir in await asyncio.to_thread(lambda: sorted(results_path.iterdir(), reverse=True)):
                if run_dir.is_dir() and await asyncio.to_thread((run_dir / "latency_summary.json").exists):
                    runs.append(run_dir.name)
                    if len(runs) >= limit:
                        break

            return runs
        except (OSError, PermissionError) as e:
            logging.error(f"Failed to get recent runs from {results_dir}: {e}")
            return []
        except Exception as e:
            logging.error(f"Unexpected error getting recent runs from {results_dir}: {e}")
            return []
